
## CloudsineAI Take Home Assignment
---
### Documentation of my work in Building a simple VirusTotal File Scanner.
---


### Contents:
- 1. Overall Problem Definition, Product Conceptualisation
- 2. Data Retrieval from VirusTotal, Understanding key information
- 3. User Workflow Ideation
- 4. Backend Data Modelling
- 5. Backend folder Structure and Architecture
- 6. Storage Considerations, and Data Persistence for Statefulness
- 7. Creating the Frontend Interface, and key UI/UX considerations
- 8. Containerising services, and CI/CD Pipline.
- 9. Docker Compose for Overall Integration, starting up project as one Complete System.
- 10. Deploying to AWS and setting up project as a secure Cloud Service. Lifecycle Analysis for Product in the long term.
---

### 1. Overall Problem Definition, Product Conceptualisation
The minimum deliverables are to create a fullstack Web Application hosted on AWS EC2, which is able to accept files, uploading them as multipart file data onto external APIs exposed by the open source VirusTotal file scanning service. Afterwards, Generative AI would be invoked to process the results generated by VirusTotal, to offer a more intuitive explanation and overview of the uploaded file's security charatheristics. 

Therefore, from here we can define three key features of this web application:
- User is able to upload files via a simple interface.
- Actual processing of security-related metadata for the uploaded file is offloaded to VirusTotal.
- After processing, Generative AI must be included as an additional service that consolidates the output from VirusTotal to a more digestable form for the lay-end user.

#### Guide to architecting the product: 10 Principles of Secure by Design
Before doing this project, I have learnt during my own free time about the 10 key Principles of designing systems. Thus, I wanted to factor in as much of those theoretical concepts into this hands-on assignment, as after all it is the foundational concepts that really drive the whole design process, and ensure that we always start off on the right foot. Therefore, regarding the scale of this project as a take home assignment and the time given, I chose to focus on these key principles:

- Principle of Least Priviledge
- Defense in Depth
- Separation of Duties
- Usability
- Minimize Attack Surface
- Secure by Default

After looking through these core concepts, I slowly prioritised where they would be most pertinent within the overall system makeup of this fullstack web application. To do so, we can model out how data would enter and leave the system, in a very simple system architecture diagram that covers all of the main components first:


```
 __________
|          |
|  Client  |  
|__________|
     |
     |
 ____|_____       ____________
|          |     |            |
|  Backend |---> | VT Service |
|          |     |____________|
|          |      __________________
|          |---> |                  |
|__________|     |  Gen AI Provider |
     |           |__________________|
 ____|_____         __________________________________________________________________________________________
|          |       |                                                                                          |
|  Storage |       | Environment Variables and Secrets, for communication between these respective components |
|   (TBC)  |       |__________________________________________________________________________________________|
|__________|

```
1. Data must be transmitted between the client and the backend server via HTTP requests and responses. 
2. If we have a storage strategy, then the backend will communicate with a chosen storage service as a client.
3. Data is fetched from VirusTotal which is the external service.
4. Key environment variables and secrets must be involved.

Therefore, from here I could slowly map out where within this system, the key concepts of security could be inculcated:

- Regarding Environment Variables and Secrets:
    - Principle of Least Priviledge:
        - We need to ensure that no other party apart from trusted interal personnel and entities, involved within the development or makeup of this system, have access to the secrets and key environment variables. Therefore, we would need to institute certain roles, and access permissions in the long run. *More will be covered in the stage of hosting on AWS.*

    - Secure By Default:
        - The environment variables and secrets should not be exposed within the codebase itself. They can be contained within .env files living inside the filesystem, but in the long run we need to do more to ensure data security, and this includes not storing any of these information within the project folder itself. Because once an attacker is able to gain access to the filesystem or should the files be leaked via wrongful repository commits, then these secrets will truly be exposed. I felt that it is also neccesary to take note at this stage: how can we easily update the secrets/keys should they be compromised? Therefore, filesystem storage via .env files are of course better than codebase exposure but more can be done, and *I will cover my journey later in the stage of hosting the completed product on AWS.*

    - Minimise Attack Surface:
        - I also believe that the less occurence of these secrets in a tangible manner, the less areas in which they can be exploited and taken.

- Data transmission between components:
    - Secure By Default:
        - I believe that even though we need to send data across from the client to the backend, we also need to take note of what data is sent. Does it need to be in this form? Is it neccesary to be this readable? Because data can always be read through packet sniffing tools like Wireshark and Burpsuite. I felt the need to minimise the meaning of the data that is sent over, especially when it is involved in repeated requests.

        - The exposure of backend APIs endpoints to be called is also a key vulnerability and attack surface. For one, we cannot just allow any source to query our backend, which can lead to all sorts of threats such as Cross-site scripting, Denial of Service (by sending a very large file and crashing our server), and many others. Cross Origin Resource Sharing is one of the ways we can guard against these, but it is far from the only method. We will also need to enforce strict data validation and sanitisation regarding the payloads that arrive to the backend server. *I will cover more of these in my actual implementation documentation.*

        - Strict Data validation does not just apply to the client and the backend. I remember learning about the Zero Trust philosophy one day, and it was "never trust, always verify". So we can also believe that what if the VirusTotal service has been spoofed, and the exposed endpoints are now sending malicious data? It can attack our backend server as well, and so we need strict data validation there too.

    - Defense in Depth:
        - Generally, I felt that at every place where our web application exposes itself to the outside world, there needs to be multiple layers of safeguards to ensure that malicious entity sources, requests are weeded out before they hit our endpoints. *I will cover more of this in the containerisation and deployment stage*

- Invoking of Generative AI
    - Secure by Default, Minimize Attack Surface and Keep it Simple:
        - Following from my past internship project which also utilised Generative AI heavily, I felt that to invoke Generative AI, the option was either to segregate the AI as an external service, running aside the backend server in AWS EC2, or have it reside within the backend server as a modular component cleanly separated in function. I experimented with the first option (separate service) during my hackathons, and it is truly a more dyanmic approach that allows for separate scalability between the backend and the AI module themselves. However, introducing the AI module as a separate service means more exposed API endpoints, and for this take home project where the role of the AI was really simple, I believed that it could be contained within the backend as a modular component, and take in the stateful data about file scan results to generate insights. This also aligns well with keeping the overall project architecture simple, and thus easily debuggable for the time being.

- Frontend, and Key Secrets Renewability:
    - Usability:
        - Following my thoughts about the overall user workflow deliverables and how it is required in the future to maintain the lifecycle of secrets and keys (if one is compromised how easily and securely can we renew them), I thus felt that the workflows for these two processes can be kept as intuitive, and industry-correct in real-life production scenarios. 

Therefore, these are some of my key considerations, tying in to my theoretical learning. From here onwards, I personally adhered to these concepts and prerequisites for security in my actual implementation.

### 2. Data Retrieval from VirusTotal

I first called the VirusTotal api endpoints via Postman, after reading their documentation. I gathered that there are 4 key endpoints that have to be used in this project:
BASE URL : https://www.virustotal.com/api/v3
POST /files --> This is for uploading a file (smaller than 32MB) 

POST /upload --> This is for uploading a file that is larger than 32MB. What is returned is a url that we then will need to use, to send the actual multipart file data via a POST request

GET /analysis --> This is to query the scan progress and thus results of a particular file sent via /files and /upload POST endpoints.

POST /files/{id} --> given a file's hash value, pertains to retrieving the pertinent reports about it.

All these endpoints require inputting the VirusTotal API key.

### 3. User Workflow Ideation
As such, from here we can spot two different workflows:

```
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   User Uploads  â”‚
                                    â”‚      File       â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚                                       â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚  Quick   â”‚                         â”‚     Full     â”‚
                    â”‚   Scan   â”‚                         â”‚     Scan     â”‚
                    â”‚ (Hash)   â”‚                         â”‚  (Upload)    â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                                       â”‚
                         â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚                           â”‚                      â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Compute  â”‚              â”‚ Small File  â”‚      â”‚   Large File    â”‚
                    â”‚SHA-256   â”‚              â”‚  â‰¤ 32MB     â”‚      â”‚  32MB-650MB     â”‚
                    â”‚  Hash    â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                     â”‚                      â”‚
                         â”‚                           â”‚                      â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚Query VT  â”‚              â”‚POST /files  â”‚      â”‚GET upload URL   â”‚
                    â”‚/files/   â”‚              â”‚             â”‚      â”‚POST to URL      â”‚
                    â”‚ {hash}   â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                     â”‚                      â”‚
                         â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                               â”‚
                    â”‚Found in  â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚VT DB?    â”‚                        â”‚   Return    â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚ Analysis ID â”‚
                         â”‚                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
              â”‚                     â”‚                          â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   YES    â”‚         â”‚     NO     â”‚          â”‚ Poll Analysis  â”‚
         â”‚Return    â”‚         â”‚  Return    â”‚          â”‚  GET /analyses â”‚
         â”‚File Obj  â”‚         â”‚  404/204   â”‚          â”‚     /{id}      â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                     â”‚                          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
                        â”‚                                      â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                   â”‚  Store         â”‚                     â”‚  Analysis   â”‚
                   â”‚  for           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Complete?  â”‚
                   â”‚  persistence   â”‚                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
                        â”‚                                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
                   â”‚         Display Results to User                 â”‚
                   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
                   â”‚    â”‚  Optional: Request AI Summary    â”‚         â”‚
                   â”‚    â”‚   (Non-blocking, Streaming)      â”‚         â”‚
                   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  User Can Navigate to File History      â”‚
                   â”‚  (Quick Retrieval from storage)         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Quick Upload via File Hash
The first workflow is for quick uploads. So if a user has a file, we can first check if the file been scanned before via a computation of its hash, and retrieve results from there. I believe this is useful for common files, malware samples that have been seen before.

#### Full Scan Upload (Small and Large Files)
The second workflow is to really send the file to VT for a full scan consisting of more than 70 machines.

I felt this constraint actually taught me something valuable about API design at scale considering how the user would like to navigate and be served. Ultimately, this can make our web application more flexible.

#### AI Summary Generation
So once the user uploads the files and gets the results, he/she can request for the AI analysis. 

#### Historical Data Retrieval
Finally, users can view the results of the files they have scanned and analysed with AI. This requirement for quick retrieval and responsiveness helped me to decide on my storage strategy, which I'll cover in Section 6.

---

### 4. Backend Data Modelling

Coming from a background where I often worked with loosely-typed data structures, I felt that this project was an opportunity to embrace strict type validation from the ground up. So I used Python's Pydantic library.

#### Why Pydantic Models?
The core principle here ties back to "Defense in Depth" and "Secure by Default". I wanted multiple layers of data validation:
1. **At the API boundary** - when data comes from the frontend
2. **At the external service boundary** - when data comes from VirusTotal
3. **At the storage boundary** - when data is retrieved from Redis

Pydantic models provide automatic validation, type checking, and clear error messages. If VirusTotal's API response structure changes unexpectedly, or if an attacker tries to send malformed data, then the Pydantic validation catches it immediately. Prevent errors from silently coming in to our system.



### 5. Backend Folder Architecture and Separation of Concerns
Therefore, st this point I organized my backend following a clear layered architecture pattern:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HTTP REQUEST (Client)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROUTES LAYER (routes/)                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  â€¢ vt_scan_files.py      - VirusTotal scan endpoints           â”‚     â”‚
â”‚  â”‚  â€¢ ai_summarise.py       - AI summary generation endpoints     â”‚     â”‚
â”‚  â”‚  â€¢ files_operations.py   - File management endpoints           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  [FastAPI validation, HTTP status codes, request/response formatting]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SERVICES LAYER (services/)                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  â€¢ vt_scan_files.py      - VT API orchestration logic          â”‚     â”‚
â”‚  â”‚  â€¢ ai_summarise.py       - AI workflow coordination            â”‚     â”‚
â”‚  â”‚  â€¢ files_operations.py   - File processing business logic      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  [Business logic, workflow orchestration, error handling]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                                     â”‚
                  â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REPOSITORY LAYER (repository/)     â”‚  â”‚  AI MODULE (ai/)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ file_uploads_record.py       â”‚ â”‚  â”‚  â”‚ â€¢ llm.py (Singleton) â”‚  â”‚
â”‚  â”‚ â€¢ parse_analysis_object_result â”‚ â”‚  â”‚  â”‚ â€¢ ai_call.py         â”‚  â”‚
â”‚  â”‚ â€¢ parse_file_object_result     â”‚ â”‚  â”‚  â”‚ â€¢ build_context.py   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚ â€¢ prompts.py         â”‚  â”‚
â”‚  [Data transformation, Redis I/O]   â”‚  â”‚  â”‚ â€¢ state.py           â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
               â”‚                         â”‚  [LangChain, LLM calls]    â”‚
               â–¼                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VT API MAPPERS (vt_api_mappers/)   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ vt_base_mapper_model.py      â”‚ â”‚
â”‚  â”‚ â€¢ vt_get_file_report.py        â”‚ â”‚
â”‚  â”‚ â€¢ vt_get_analysis.py           â”‚ â”‚
â”‚  â”‚ â€¢ vt_post_upload_file.py       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  [Pydantic models for VT API]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DOMAIN LAYER (domain/)             â”‚  â”‚  UTILS (utils/)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ file_uploads_record.py       â”‚ â”‚  â”‚  â”‚ â€¢ config.py             â”‚  â”‚
â”‚  â”‚   (Core business model)        â”‚ â”‚  â”‚  â”‚   (Singleton)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚ â€¢ vt_redis_cache.py     â”‚  â”‚
â”‚  [Pydantic domain models]           â”‚  â”‚  â”‚ â€¢ generate_file_hash.py â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â€¢ temp_storage.py       â”‚  â”‚
                                         â”‚  â”‚ â€¢ streaming_queue.py    â”‚  â”‚
                                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                         â”‚  [Shared functionalities]     â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”
â”‚              EXTERNAL SERVICES & STORAGE               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  VirusTotal  â”‚  â”‚    Data      â”‚  â”‚  OpenAI API â”‚   â”‚
â”‚  â”‚     API      â”‚  â”‚    Store     â”‚  â”‚             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

backend/
â”œâ”€â”€ routes/           # Expose APIs
â”œâ”€â”€ services/         # Where core business logic exists, and utilise the methods exposed by the repository.
â”œâ”€â”€ repository/       # Data access, so we dont access the domain class model directly.
â”œâ”€â”€ domain/           # Core domain model (FileUploadsRecord Singleton)
â”œâ”€â”€ vt_api_mappers/   # VirusTotal API response models, and also the entity classes for the data passed over.
â”œâ”€â”€ ai/               # AI module (LangChain, LangGraph + LLM)
â””â”€â”€ utils/            # Shared utilities and config
```

Each layer has a distinct responsibility:

- **`domain/`**: Contains the Pydantic models representing core business entities. For example, `file_uploads_record.py` is the Singleton class that contains all data that needs to be persisted. It represents itself as a boundary actor between the backend server and the datastore, fetching data from the datastore and thus only encapsulating the current data the client wants, and of which the end user is looking at. Therefore, also represents itself as the Single Source of Truth for the AI, in terms of what to analyse.

- **`vt_api_mappers/`**: Contains Pydantic models that map exactly to VirusTotal's API response schemas. Entity class models like `vt_get_file_report.py`, `vt_get_analysis.py`, and `vt_post_upload_file.py` ensure we can safely parse and validate responses from VirusTotal. A translation layer between VT and our backend server.

- **`repository/`**: This layer handles data transformation and access. Files like `parse_analysis_object_result.py` and `parse_file_object_result.py` take the raw results from VT, and parse them to be represented one layer down in the above described entity classes. The other main repository file `file_uploads_record.py` is syntactically defined like its domain class model counterpart. I felt that this provides more clarity so developers can easily map out which files belong to which group as an architectural set. `file_uploads_record.py` exposes key methods that manipulate the domain attributes in `file_uploads_record.py`.

- **`services/`**: The core business logic, like calling VirusTotal APIs, invoking the repository methods to store data, and manage error handling.

- **`routes/`**: These are the FastAPI route handlers that define our REST API endpoints, and the entry point.

For further clarity I input my debug statements in a particular format : `[FILENAME_LAYER <statement>]`

#### Singleton Pattern for State Management
2. **file_uploads_record.py**

Mainly, it contains info about:
- The current upload result (vt_upload_result)
- The current AI summary 
- A hashmap consisting of all filenames, to analysis Ids (for the full scan)
- A hashmap consisting of all UUIDs (generated via uuid.uuid3() so each unique filename is given a unique UUID).

*So linking back to my considerations about data transmissions being as less meaningful as possible*, so even with packet sniffing attackers cannot get much out of anything --> whenever a user chooses to retrieve past data regarding a certain filename, then the UUID is sent from the client to the backend server instead of the filename. 

*A UUID also makes all of the data much more controllable semantically*, and how unique entries are created is through a small edit I created at the *services* layer, which *calculates the UUID*: For the filename is concatenated with `quick` or `full` so the resulting UUID is different for a filename that was sent to quick scan, and full scan. 


**2. LLM Instance**
The Generative AI model is encapsulated in a singleton (`ai/llm.py`). This design decision came from my internship experience.

By using a singleton with a clean interface, we can easily interchange the underlying GenAI service without changing any of the code that calls it, something like dependency injection.

**3. Global Settings Configuration**
Following the principle of "Secure by Default", I centralized all configuration in a singleton `SystemSettings` class (`utils/config.py`). This singleton loads environment variables once at startup and provides typed access to:
- API keys (VirusTotal, OpenAI/Anthropic)
- Redis connection details
- File size limits
- Temporary storage paths
---

### 6. Storage Considerations and Data Persistence

Now on to data storage and persistence. One of my key inspirations for this project (and how it would influence my frotend design) is *remove.bg* website. User's ultimately can view their old files which were processed, and now when we have GenAI, we also need to make sure that we keep the data alive for usage by the GenAI.

#### Why Not In-Memory (Process Memory)?
I knew that the simplest way would be to store the data in-memory via data structures like hashmaps. But I felt that this has its limitations especially when *creating a service that needs to scale in the future:*

1. **Data Volume**: VirusTotal returns comprehensive analysis objects with data from 70+ antivirus engines, file metadata, threat classifications, and more. A single file report can be several kilobytes of JSON. Add AI summaries on top of that and there will be a lot of data residing in process memory.

2. **Persistence**: If the backend server restarts, all scan history is lost also.

3. **Scalability**: In-memory storage doesn't scale horizontally, and so multiple EC2 instances cannot share state. If a user is directed between two web servers then he/she may get inconsistent data.

Regarding this project, I felt that a traditional database would be too overkill and complex as well, because then we would need to focus on relational data mapping. And *going back to simplicity*, I felt that there is *no need for a user to log in* to our website. Similar to *remove.bg*, we just offer good quality insights. 


#### Redis as the Sweet Spot
So I chose Redis, a single threaded in-memory database. which offers *very fast retrieval*.

I implemented Redis caching as a separate utlity class, and its exposed methods are called by the `file_uploads_record` singleton through the repository. Pattern:
    1. Check if result exists in Redis cache by UUID
    2. If cache hit, return immediately
    3. If cache miss, fetch from VirusTotal, store in cache, return result
---

### 7. Creating the Frontend Interface

Following my design, security considerations and inspiration by **remove.bg** until here, I decided to make the frontend a simple Single Page Application (SPA).

#### Why Material UI and React?
For this project, I decided to step out of my comfort zone and learn React with Material UI (MUI). My experience in frontend developement has been vanilla HTML, CSS, ReactTS/JS. But considering my time, using a framework was *much faster for iteration, and also makes my code easier to be understood across a unified standard, by other developers.*

#### Organizing Logic: The Hooks Pattern
One pattern that really clicked for me was custom hooks. Coming from vanilla JavaScript where I'd often have tangled code mixing API calls, UI updates, and state management, React hooks felt like a revelation in separation of concerns.


#### Importance of UI flow:
As such, according to the user workflow diagrams the UI should be fast, and while waiting for a full scan to complete / AI to generate analysis the user should be able to browse other things.

From my internship experienceI have realised how slow LLM inference can be, so its good to make this part *non-blocking*. 
Flow:
    1. The AI summary request is sent to the backend
    2. The backend streams chunks of the summary as they're generated
    3. The frontend displays the summary progressively, character by character
    4. Meanwhile, the user can navigate away, view other files, or continue using the application.

---

### 8. Containerization and Docker, Nginx.

#### Secure Temporary File Handling
I realised that as FastAPI processes uploaded files, they're still temporarily stored in server memory and potentially written to disk in temp directories. So, what we need to make sure that these files dont get executed. 

Drawing from the "Secure by Default" principle, I implemented a secure temporary storage strategy:

1. **Dedicated Temp Directory**: Create a specific directory (`/tmp/file_uploads` in the container) for uploaded files
2. **Restricted Permissions**: In the Dockerfile, explicitly remove write and execute permissions from this directory for uploaded content
3. **Immediate Cleanup**: Files are read into memory and the temp file is deleted immediately after processing

Dockerfile (Backend):
```
# Create a dedicated non-root user for security
RUN useradd --uid 10001 --create-home --shell /bin/bash appuser && \
    mkdir -p ${TMP_UPLOAD_DIR} && \
    chown -R appuser:appuser /app && \
    chmod 700 ${TMP_UPLOAD_DIR}

Over here, I made sure to create the temporary folder, with initial permissions set as ------ so only appuser can access the folder, read, write, and can traverse into it.
```
Then in docker compose:
```
tmpfs:
      - /app/tmp_uploads:rw,noexec,nosuid,size=100m,uid=10001,gid=10001,mode=1777
    restart: unless-stopped

Over here we make sure that the same folder has no permissions to execute files inside
```

This ensures that even if someone uploads a malicious executable, it cannot be executed from the temp directory. The file permissions act as an additional security boundary.

#### Data transmission flow from client -> Nginx -> backend server.

One thing I really wanted to emphasize is validation at multiple layers. I implemented file size limits in three places:

1. **Nginx Configuration** (`client/nginx.conf`):
   ```nginx
   client_max_body_size 650m;
   ```
   This is the first line of defense. Nginx rejects oversized uploads before they even reach the backend.

2. **FastAPI Route Handler** (`backend/routes/vt_scan_files.py`):
   ```python
   VT_MAX_FILE_SIZE = 650 * 1024 * 1024  # 650MB
   if file.size > VT_MAX_FILE_SIZE:
       raise HTTPException(...)
   ```
   Even if something bypasses Nginx, the route handler checks again.

3. **Service Layer** (`backend/services/vt_scan_files.py`):
   ```python
   content_size = len(content)
   if content_size > 650 * 1024 * 1024:
       raise HTTPException(...)
   ```
   Finally, the service validates the actual content size.

This redundancy felt important, as each layer doesn't trust the previous one. I applied the "Zero Trust" principle here. If there's a misconfiguration at one layer, the others catch it.

**Actual Configuration Snippets:**

1. **Nginx Layer** (`client/nginx.conf`):
```nginx
server {
    listen 80;
    server_name _;

    # ğŸ” Upload size (VirusTotal max is 650MB)
    client_max_body_size 650m;

    # Backend API proxy
    location /api/ {
        proxy_pass http://backend:8000/;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # ğŸ”‘ Required for file uploads
        proxy_request_buffering off;
        proxy_buffering off;
    }
}
```

2. **FastAPI Route Layer** (`backend/routes/vt_scan_files.py`):
```python
@router.post("/upload-complete")
async def full_scan(file: UploadFile = File(...), password: Optional[str] = None):
    # VirusTotal supports files up to 650MB
    VT_MAX_FILE_SIZE = 650 * 1024 * 1024  # 650MB
    
    if file.size > VT_MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail="File size exceeds 650MB limit. VirusTotal does not support files larger than this."
        )
    
    result = await upload_file_to_vt_db(file=file, password=password)
    return {"status": status.HTTP_200_OK, "result": result}
```

3. **Service Layer** (`backend/services/vt_scan_files.py`):
```python
async def upload_file_to_vt_db(file: UploadFile, password: str):
    content = await file.read()
    content_size = len(content)
    
    # Check if file is too large (650MB limit as per VT documentation)
    if content_size > 650 * 1024 * 1024:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail="[VT_SCAN_FILES_SERVICE] File size exceeds 650MB limit."
        )
    
    # For files larger than 32MB, use special upload URL
    if content_size > 32 * 1024 * 1024:
        upload_url = await get_upload_url_for_large_file()
        analysis_id = await upload_large_file_to_vt(file, content, upload_url, password)
        # ... rest of logic
```

#### Nginx other configurations:
Other functionalities set in Nginx:

- **Request Buffering**: Disabled (`proxy_request_buffering off`) for efficient file upload streaming
- **Static Asset Caching**: Caches frontend assets to reduce load
- **API Routing**: Cleanly separates `/api/*` routes from the backend, so we dont expose the backend directly.
---

### 9. Docker Compose for Overall Integration

With three separate services (client, backend, Redis), I thus proceeded to integrate all of them under one startable platform via Docker Compose:

`docker-compose.yml`:

```yaml
services:
  backend:
    # Python FastAPI service
    # Depends on Redis
    
  redis:
    # Caching layer
    # Persistent volume for data
    
  client:
    # Nginx + React frontend
    # Depends on backend
```
**Port Mapping**: Only port 80 (Nginx) is exposed to the host. Backend and Redis are accessible only within the Docker network. This is also present when I set the *Security Groups* in AWS.

Running `docker-compose up` brings up the entire application stack. This made development and testing so much smoother - I could tear down and rebuild the whole system with one command.

---

### 10. Deploying to AWS and Secure Cloud Architecture

#### The Secrets Manager Migration
Earlier, I mentioned that using `.env` files for secrets was acceptable for development but not for production. When deploying to AWS EC2, I implemented proper secrets management using AWS Secrets Manager.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SECRETS LIFECYCLE MANAGEMENT                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         DEVELOPMENT                          PRODUCTION (AWS)
              
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   .env file     â”‚                  â”‚  AWS Secrets Manager   â”‚
    â”‚  (gitignored)   â”‚                  â”‚   (vt-scanner-secrets) â”‚
    â”‚                 â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚ VT_API_KEY=xxx  â”‚                  â”‚  â”‚ VT_API_KEY      â”‚   â”‚
    â”‚ OPENAI_KEY=yyy  â”‚                  â”‚  â”‚ OPENAI_API_KEY  â”‚   â”‚
    â”‚ REDIS_HOST=...  â”‚                  â”‚  â”‚ REDIS_HOST      â”‚   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚ REDIS_PORT      â”‚   â”‚
             â”‚                           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
             â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                      â”‚
             â–¼                                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Backend reads    â”‚              â”‚     IAM Role         â”‚
    â”‚   from .env on     â”‚              â”‚  (EC2 Instance)      â”‚
    â”‚    startup         â”‚              â”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  Policy:             â”‚
                                        â”‚  SecretsManager      â”‚
                                        â”‚  ReadWrite           â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  Deployment Script   â”‚
                                        â”‚  (deploy_ec2.sh)     â”‚
                                        â”‚                      â”‚
                                        â”‚  aws secretsmanager  â”‚
                                        â”‚  get-secret-value    â”‚
                                        â”‚  â”œâ”€â–º export VT_*     â”‚
                                        â”‚  â”œâ”€â–º export OPENAI_* â”‚
                                        â”‚  â””â”€â–º export REDIS_*  â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  Docker Compose      â”‚
                                        â”‚  up -d --build       â”‚
                                        â”‚                      â”‚
                                        â”‚  Backend reads env   â”‚
                                        â”‚  vars (same as dev)  â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Example workflow (Key Compromised):

    âš ï¸  VT API Key Compromised!
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. Generate new    â”‚
    â”‚    key in VT       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 2. Update secret   â”‚
    â”‚    in AWS Secrets  â”‚         
    â”‚    Manager console â”‚        
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 3. Restart backend â”‚
    â”‚    service via:    â”‚
    â”‚    ./deploy_ec2.sh â”‚
    â”‚    (re-runs script)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 4. Script fetches  â”‚
    â”‚    new secret and  â”‚
    â”‚    redeploys       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The journey went like this:

1. **Created Secrets in AWS Secrets Manager**: Stored `VT_API_KEY`, `OPENAI_API_KEY`, `REDIS_PASSWORD`.

2. **IAM Role for EC2**: Created an IAM role with `SecretsManagerReadWrite` policy and attached it to the EC2 instance. This embodies the Principle of Least Privilege - the EC2 instance can read secrets, and only the secrets of this file scanner --> vtfilescanner via its `Amazon Resource Name (arn)` provided into the policy JSON.

3. **Deployment Script Fetches Secrets**: The deployment script (`scripts/deploy_ec2.sh`) then fetches the secrets from Secrets Manager using AWS CLI once during startup, then exports them as environment variables:
   
   **Deployment Script** (`scripts/deploy_ec2.sh`):
   ```bash
   #!/usr/bin/env bash
   # Deploy ProtoCyber stack on EC2 using AWS Secrets Manager
   
   SECRET_ID=${1:-Protocyber/BackendSecrets}
   REPO_DIR="/home/ec2-user/protocyber128_Take_Home_Assignment"
   
   # Check for required tools
   if ! command -v aws >/dev/null 2>&1; then
     echo "aws CLI not found. Install awscli v2." >&2
     exit 1
   fi
   
   if ! command -v jq >/dev/null 2>&1; then
     echo "jq not found. Install it with 'sudo yum install -y jq'." >&2
     exit 1
   fi
   
   # Fetch secrets as a JSON document from AWS Secrets Manager
   SECRET_JSON=$(aws secretsmanager get-secret-value \
     --secret-id "$SECRET_ID" \
     --query SecretString --output text)
   
   # Parse JSON and export as environment variables
   export VT_API_KEY=$(echo "$SECRET_JSON" | jq -r '.VT_API_KEY')
   export OPENAI_API_KEY=$(echo "$SECRET_JSON" | jq -r '.OPENAI_API_KEY')
   export REDIS_PASSWORD=$(echo "$SECRET_JSON" | jq -r '.REDIS_PASSWORD')
   export REDIS_USERNAME=${REDIS_USERNAME:-"default"}
   export REDIS_HOST=${REDIS_HOST:-"redis"}
   export REDIS_PORT=${REDIS_PORT:-6379}
   
   cd "$REPO_DIR"
   
   # Pull latest code
   git fetch --prune
   git checkout main
   git pull --ff-only origin main
   
   # Rebuild and start containers with secrets as environment variables
   docker compose up -d --build
   
   echo "Deployment complete!"
   docker compose ps
   ```
*So there are no Secrets in Repository*, and the `.env` file is gitignored. Even if the repository is compromised, no secrets are exposed. The deployment script runs on the EC2 instance which has the IAM role attached, so it can fetch secrets without any credentials stored on disk.


#### EC2 Security Configuration
Beyond secrets management, I configured the EC2 instance with:

- **Security Groups**: Inbound rules configured as follows:
  - **SSH (port 22)**: From `115.66.154.71/32` only - restricted to my IP address for secure management
  - **HTTP (port 80)**: Two rules:
    - From `115.66.154.71/32` - for backend interface access
    - From `0.0.0.0/0` - for public frontend access
  - **Custom TCP (port 4173)**: From `0.0.0.0/0` - for development/testing purposes (Vite dev server)
  - All other ports are blocked by default

#### Github Actions
Finally, I also implemented a simple github actions CI/CD workflow that on every push, both the frontend and backend codes will be built and then containerised using Docker. Testing can be added in the long term as well. So on every push subsequently, I could see from Github which pushes actually failed and due to silly bugs.

Overall, this was a very englightening project, and I have learnt a lot. Looking forward to the discussion!

---



