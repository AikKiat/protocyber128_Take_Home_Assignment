
## CloudsineAI Take Home Assignment
---
### Documentation of my work in Building a simple VirusTotal File Scanner.
---


### Contents:
- 1. Overall Problem Definition, Product Conceptualisation
- 2. Data Retrieval from VirusTotal, Understanding key information
- 3. User Workflow Ideation
- 4. Backend Data Modelling
- 5. Backend folder Structure and Architecture
- 6. Storage Considerations, and Data Persistence for Statefulness
- 7. Creating the Frontend Interface, and key UI/UX considerations
- 8. Containerising services, and CI/CD Pipline.
- 9. Docker Compose for Overall Integration, starting up project as one Complete System.
- 10. Deploying to AWS and setting up project as a secure Cloud Service. Lifecycle Analysis for Product in the long term.
---

### 1. Overall Problem Definition, Product Conceptualisation
The minimum deliverables are to create a fullstack Web Application hosted on AWS EC2, which is able to accept files, uploading them as multipart file data onto external APIs exposed by the open source VirusTotal file scanning service. Afterwards, Generative AI would be invoked to process the results generated by VirusTotal, to offer a more intuitive explanation and overview of the uploaded file's security charatheristics. 

Therefore, from here we can define three key features of this web application:
- User is able to upload files via a simple interface.
- Actual processing of security-related metadata for the uploaded file is offloaded to VirusTotal.
- After processing, Generative AI must be included as an additional service that consolidates the output from VirusTotal to a more digestable form for the lay-end user.

#### Guide to architecting the product: 10 Principles of Secure by Design
Before doing this project, I have learnt during my own free time about the 10 key Principles of designing systems. Thus, I wanted to factor in as much of those theoretical concepts into this hands-on assignment, as after all it is the foundational concepts that really drive the whole design process, and ensure that we always start off on the right foot. Therefore, regarding the scale of this project as a take home assignment and the time given, I chose to focus on these key principles:

- Principle of Least Priviledge
- Defense in Depth
- Separation of Duties
- Usability
- Minimize Attack Surface
- Secure by Default

After looking through these core concepts, I slowly prioritised where they would be most pertinent within the overall system makeup of this fullstack web application. To do so, we can model out how data would enter and leave the system, in a very simple system architecture diagram that covers all of the main components first:


```
 __________
|          |
|  Client  |  
|__________|
     |
     |
 ____|_____       ____________
|          |     |            |
|  Backend |---> | VT Service |
|          |     |____________|
|          |      __________________
|          |---> |                  |
|__________|     |  Gen AI Provider |
     |           |__________________|
 ____|_____         __________________________________________________________________________________________
|          |       |                                                                                          |
|  Storage |       | Environment Variables and Secrets, for communication between these respective components |
|   (TBC)  |       |__________________________________________________________________________________________|
|__________|

```
1. Data must be transmitted between the client and the backend server via HTTP requests and responses. 
2. If we have a storage strategy, then the backend will communicate with a chosen storage service as a client.
3. Data is fetched from VirusTotal which is the external service.
4. Key environment variables and secrets must be involved.

Therefore, from here I could slowly map out where within this system, the key concepts of security could be inculcated:

- Regarding Environment Variables and Secrets:
    - Principle of Least Priviledge:
        - We need to ensure that no other party apart from trusted interal personnel and entities, involved within the development or makeup of this system, have access to the secrets and key environment variables. Therefore, we would need to institute certain roles, and access permissions in the long run. *More will be covered in the stage of hosting on AWS.*

    - Secure By Default:
        - The environment variables and secrets should not be exposed within the codebase itself. They can be contained within .env files living inside the filesystem, but in the long run we need to do more to ensure data security, and this includes not storing any of these information within the project folder itself. Because once an attacker is able to gain access to the filesystem or should the files be leaked via wrongful repository commits, then these secrets will truly be exposed. I felt that it is also neccesary to take note at this stage: how can we easily update the secrets/keys should they be compromised? Therefore, filesystem storage via .env files are of course better than codebase exposure but more can be done, and *I will cover my journey later in the stage of hosting the completed product on AWS.*

    - Minimise Attack Surface:
        - I also believe that the less occurence of these secrets in a tangible manner, the less areas in which they can be exploited and taken.

- Data transmission between components:
    - Secure By Default:
        - I believe that even though we need to send data across from the client to the backend, we also need to take note of what data is sent. Does it need to be in this form? Is it neccesary to be this readable? Because data can always be read through packet sniffing tools like Wireshark and Burpsuite. I felt the need to minimise the meaning of the data that is sent over, especially when it is involved in repeated requests.

        - The exposure of backend APIs endpoints to be called is also a key vulnerability and attack surface. For one, we cannot just allow any source to query our backend, which can lead to all sorts of threats such as Cross-site scripting, Denial of Service (by sending a very large file and crashing our server), and many others. Cross Origin Resource Sharing is one of the ways we can guard against these, but it is far from the only method. We will also need to enforce strict data validation and sanitisation regarding the payloads that arrive to the backend server. *I will cover more of these in my actual implementation documentation.*

        - Strict Data validation does not just apply to the client and the backend. I remember learning about the Zero Trust philosophy one day, and it was "never trust, always verify". So we can also believe that what if the VirusTotal service has been spoofed, and the exposed endpoints are now sending malicious data? It can attack our backend server as well, and so we need strict data validation there too.

    - Defense in Depth:
        - Generally, I felt that at every place where our web application exposes itself to the outside world, there needs to be multiple layers of safeguards to ensure that malicious entity sources, requests are weeded out before they hit our endpoints. *I will cover more of this in the containerisation and deployment stage*

- Invoking of Generative AI
    - Secure by Default, Minimize Attack Surface and Keep it Simple:
        - Following from my past internship project which also utilised Generative AI heavily, I felt that to invoke Generative AI, the option was either to segregate the AI as an external service, running aside the backend server in AWS EC2, or have it reside within the backend server as a modular component cleanly separated in function. I experimented with the first option (separate service) during my hackathons, and it is truly a more dyanmic approach that allows for separate scalability between the backend and the AI module themselves. However, introducing the AI module as a separate service means more exposed API endpoints, and for this take home project where the role of the AI was really simple, I believed that it could be contained within the backend as a modular component, and take in the stateful data about file scan results to generate insights. This also aligns well with keeping the overall project architecture simple, and thus easily debuggable for the time being.

- Frontend, and Key Secrets Renewability:
    - Usability:
        - Following my thoughts about the overall user workflow deliverables and how it is required in the future to maintain the lifecycle of secrets and keys (if one is compromised how easily and securely can we renew them), I thus felt that the workflows for these two processes can be kept as intuitive, and industry-correct in real-life production scenarios. 

Therefore, these are some of my key considerations, tying in to my theoretical learning. From here onwards, I personally adhered to these concepts and prerequisites for security in my actual implementation.

### 2. 



